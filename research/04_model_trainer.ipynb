{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: filelock in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Found existing installation: transformers 4.33.1\n",
      "Uninstalling transformers-4.33.1:\n",
      "  Successfully uninstalled transformers-4.33.1\n",
      "Found existing installation: accelerate 0.22.0\n",
      "Uninstalling accelerate-0.22.0:\n",
      "  Successfully uninstalled accelerate-0.22.0\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/13/30/54b59e73400df3de506ad8630284e9fd63f4b94f735423d55fc342181037/transformers-4.33.1-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.33.1-py3-none-any.whl.metadata (119 kB)\n",
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/4d/a7/05c67003d659a0035f2b3a8cf389c1d9645865aee84a73ce99ddab16682f/accelerate-0.22.0-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: psutil in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: fsspec in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects and work\\projects\\nlp\\nlp projects\\text_summarization\\end_to_end_text_summarizer\\virt\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "Using cached accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "Installing collected packages: transformers, accelerate\n",
      "Successfully installed accelerate-0.22.0 transformers-4.33.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate\n",
    "!pip uninstall -y transformers accelerate\n",
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects and Work\\\\Projects\\\\NLP\\\\NLP PROJECTS\\\\Text_Summarization\\\\End_to_End_Text_Summarizer\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects and Work\\\\Projects\\\\NLP\\\\NLP PROJECTS\\\\Text_Summarization\\\\End_to_End_Text_Summarizer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating entities \n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size:int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration Manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Configuration Manager \n",
    "from TextSummarizer.constants import *\n",
    "from TextSummarizer.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            num_train_epochs = params.num_train_epochs,\n",
    "            warmup_steps = params.warmup_steps,\n",
    "            per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size = params.per_device_eval_batch_size,\n",
    "            weight_decay = params.weight_decay,\n",
    "            logging_steps = params.logging_steps,\n",
    "            evaluation_strategy = params.evaluation_strategy,\n",
    "            eval_steps = params.evaluation_strategy,\n",
    "            save_steps = params.save_steps,\n",
    "            gradient_accumulation_steps = params.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating components \n",
    "from transformers import TrainingArguments, Trainer \n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PegasusTokenizerFast\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config:ModelTrainerConfig):\n",
    "        self.config= config\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collector = DataCollatorForSeq2Seq(tokenizer,model= model_pegasus)\n",
    "\n",
    "        ## Loading dataset\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "        # trainer_args = TrainingArguments(\n",
    "        #     output_dir=self.config.root_dir, \n",
    "        #     num_train_epochs=self.config.num_train_epochs, \n",
    "        #     warmup_steps=self.config.warmup_steps,\n",
    "        #     per_device_train_batch_size=self.config.per_device_train_batch_size, \n",
    "        #     per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "        #     weight_decay=self.config.weight_decay,\n",
    "        #     logging_steps=self.config.logging_steps,\n",
    "        #     evaluation_strategy=self.config.evaluation_strategy,\n",
    "        #       eval_steps=self.config.eval_steps, save_steps=1e6,\n",
    "        #     gradient_accumulation_steps=self.config.gradient_accumulation_steps\n",
    "        # ) \n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir, num_train_epochs=1, warmup_steps=500,\n",
    "            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "            weight_decay=0.01, logging_steps=10,\n",
    "            evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "            gradient_accumulation_steps=16\n",
    "            )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        trainer= Trainer(model= model_pegasus, \n",
    "                         args= trainer_args,\n",
    "                         tokenizer= tokenizer,\n",
    "                         data_collator=seq2seq_data_collector,\n",
    "                         train_dataset= dataset_samsum_pt[\"test\"],\n",
    "                         eval_dataset= dataset_samsum_pt[\"validation\"]\n",
    "                         )\n",
    "        trainer.train()\n",
    "\n",
    "        ## Saving the model \n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n",
    "\n",
    "        ## Saving tokenizer \n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the three commands from the main text summarizer file in the terminal \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-13 21:58:59,386:INFO:common:yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-09-13 21:58:59,389:INFO:common:yaml file: params.yaml loaded successfully]\n",
      "[2023-09-13 21:58:59,389:INFO:common:Created directories at : artifacts]\n",
      "[2023-09-13 21:58:59,390:INFO:common:Created directories at : artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "'D:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\mlruns' does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\research\\04_model_trainer.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_trainer_config\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\research\\04_model_trainer.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_trainer_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_model_trainer_config()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_trainer_config \u001b[39m=\u001b[39m ModelTrainer(config\u001b[39m=\u001b[39mmodel_trainer_config)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_trainer_config\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\research\\04_model_trainer.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m trainer_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mroot_dir, num_train_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, warmup_steps\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     gradient_accumulation_steps\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m trainer\u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39m model_pegasus, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                  args\u001b[39m=\u001b[39m trainer_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                  tokenizer\u001b[39m=\u001b[39m tokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m                  eval_dataset\u001b[39m=\u001b[39m dataset_samsum_pt[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m                  )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m## Saving the model \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20Work/Projects/NLP/NLP%20PROJECTS/Text_Summarization/End_to_End_Text_Summarizer/research/04_model_trainer.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m model_pegasus\u001b[39m.\u001b[39msave_pretrained(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mroot_dir,\u001b[39m\"\u001b[39m\u001b[39mpegasus-samsum-model\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\transformers\\trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1554\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1555\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1556\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1557\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1558\u001b[0m     )\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\transformers\\trainer.py:1778\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step\n\u001b[0;32m   1776\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m-> 1778\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_train_begin(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[0;32m   1780\u001b[0m \u001b[39m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mignore_data_skip:\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\transformers\\trainer_callback.py:362\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[1;34m(self, args, state, control)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[0;32m    361\u001b[0m     control\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_begin\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\transformers\\trainer_callback.py:406\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[1;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    405\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m--> 406\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(callback, event)(\n\u001b[0;32m    407\u001b[0m             args,\n\u001b[0;32m    408\u001b[0m             state,\n\u001b[0;32m    409\u001b[0m             control,\n\u001b[0;32m    410\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[0;32m    411\u001b[0m             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[0;32m    412\u001b[0m             optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[0;32m    413\u001b[0m             lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler,\n\u001b[0;32m    414\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader,\n\u001b[0;32m    415\u001b[0m             eval_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_dataloader,\n\u001b[0;32m    416\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    417\u001b[0m         )\n\u001b[0;32m    418\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:1017\u001b[0m, in \u001b[0;36mMLflowCallback.on_train_begin\u001b[1;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args, state, control, model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1016\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized:\n\u001b[1;32m-> 1017\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(args, state, model)\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:986\u001b[0m, in \u001b[0;36mMLflowCallback.setup\u001b[1;34m(self, args, state, model)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_name:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Use of set_experiment() ensure that Experiment is created if not exists\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ml_flow\u001b[39m.\u001b[39mset_experiment(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_name)\n\u001b[1;32m--> 986\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ml_flow\u001b[39m.\u001b[39;49mstart_run(run_name\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mrun_name, nested\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nested_run)\n\u001b[0;32m    987\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMLflow run started with run_id=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ml_flow\u001b[39m.\u001b[39mactive_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    988\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_auto_end_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:350\u001b[0m, in \u001b[0;36mstart_run\u001b[1;34m(run_id, experiment_id, run_name, nested, tags, description)\u001b[0m\n\u001b[0;32m    346\u001b[0m         user_specified_tags[MLFLOW_RUN_NAME] \u001b[39m=\u001b[39m run_name\n\u001b[0;32m    348\u001b[0m     resolved_tags \u001b[39m=\u001b[39m context_registry\u001b[39m.\u001b[39mresolve_tags(user_specified_tags)\n\u001b[1;32m--> 350\u001b[0m     active_run_obj \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcreate_run(\n\u001b[0;32m    351\u001b[0m         experiment_id\u001b[39m=\u001b[39;49mexp_id_for_run, tags\u001b[39m=\u001b[39;49mresolved_tags, run_name\u001b[39m=\u001b[39;49mrun_name\n\u001b[0;32m    352\u001b[0m     )\n\u001b[0;32m    354\u001b[0m _active_run_stack\u001b[39m.\u001b[39mappend(ActiveRun(active_run_obj))\n\u001b[0;32m    355\u001b[0m \u001b[39mreturn\u001b[39;00m _active_run_stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\mlflow\\tracking\\client.py:275\u001b[0m, in \u001b[0;36mMlflowClient.create_run\u001b[1;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_run\u001b[39m(\n\u001b[0;32m    225\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    226\u001b[0m     experiment_id: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     run_name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Run:\n\u001b[0;32m    231\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m    Create a :py:class:`mlflow.entities.Run` object that can be associated with\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m    metrics, parameters, artifacts, etc.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39m        status: RUNNING\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mcreate_run(experiment_id, start_time, tags, run_name)\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:131\u001b[0m, in \u001b[0;36mTrackingServiceClient.create_run\u001b[1;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m# Extract user from tags\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[39m# This logic is temporary; the user_id attribute of runs is deprecated and will be removed\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m# in a later release.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m user_id \u001b[39m=\u001b[39m tags\u001b[39m.\u001b[39mget(MLFLOW_USER, \u001b[39m\"\u001b[39m\u001b[39munknown\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mcreate_run(\n\u001b[0;32m    132\u001b[0m     experiment_id\u001b[39m=\u001b[39;49mexperiment_id,\n\u001b[0;32m    133\u001b[0m     user_id\u001b[39m=\u001b[39;49muser_id,\n\u001b[0;32m    134\u001b[0m     start_time\u001b[39m=\u001b[39;49mstart_time \u001b[39mor\u001b[39;49;00m get_current_time_millis(),\n\u001b[0;32m    135\u001b[0m     tags\u001b[39m=\u001b[39;49m[RunTag(key, value) \u001b[39mfor\u001b[39;49;00m (key, value) \u001b[39min\u001b[39;49;00m tags\u001b[39m.\u001b[39;49mitems()],\n\u001b[0;32m    136\u001b[0m     run_name\u001b[39m=\u001b[39;49mrun_name,\n\u001b[0;32m    137\u001b[0m )\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:579\u001b[0m, in \u001b[0;36mFileStore.create_run\u001b[1;34m(self, experiment_id, user_id, start_time, tags, run_name)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[39mCreates a run with the specified attributes.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m experiment_id \u001b[39m=\u001b[39m FileStore\u001b[39m.\u001b[39mDEFAULT_EXPERIMENT_ID \u001b[39mif\u001b[39;00m experiment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m experiment_id\n\u001b[1;32m--> 579\u001b[0m experiment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_experiment(experiment_id)\n\u001b[0;32m    580\u001b[0m \u001b[39mif\u001b[39;00m experiment \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[0;32m    582\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not create run under experiment with ID \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m - no such experiment \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexists.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m experiment_id,\n\u001b[0;32m    584\u001b[0m         databricks_pb2\u001b[39m.\u001b[39mRESOURCE_DOES_NOT_EXIST,\n\u001b[0;32m    585\u001b[0m     )\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:410\u001b[0m, in \u001b[0;36mFileStore.get_experiment\u001b[1;34m(self, experiment_id)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[39mFetch the experiment.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39mNote: This API will search for active as well as deleted experiments.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39m:return: A single Experiment object if it exists, otherwise raises an Exception.\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    409\u001b[0m experiment_id \u001b[39m=\u001b[39m FileStore\u001b[39m.\u001b[39mDEFAULT_EXPERIMENT_ID \u001b[39mif\u001b[39;00m experiment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m experiment_id\n\u001b[1;32m--> 410\u001b[0m experiment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_experiment(experiment_id)\n\u001b[0;32m    411\u001b[0m \u001b[39mif\u001b[39;00m experiment \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[0;32m    413\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExperiment \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m experiment_id,\n\u001b[0;32m    414\u001b[0m         databricks_pb2\u001b[39m.\u001b[39mRESOURCE_DOES_NOT_EXIST,\n\u001b[0;32m    415\u001b[0m     )\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:375\u001b[0m, in \u001b[0;36mFileStore._get_experiment\u001b[1;34m(self, experiment_id, view_type)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_experiment\u001b[39m(\u001b[39mself\u001b[39m, experiment_id, view_type\u001b[39m=\u001b[39mViewType\u001b[39m.\u001b[39mALL):\n\u001b[1;32m--> 375\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_root_dir()\n\u001b[0;32m    376\u001b[0m     _validate_experiment_id(experiment_id)\n\u001b[0;32m    377\u001b[0m     experiment_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_experiment_path(experiment_id, view_type)\n",
      "File \u001b[1;32md:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\virt\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:179\u001b[0m, in \u001b[0;36mFileStore._check_root_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39mRun checks before running directory operations.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_directory):\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_directory)\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_directory):\n\u001b[0;32m    181\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a directory.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_directory)\n",
      "\u001b[1;31mException\u001b[0m: 'D:\\Projects and Work\\Projects\\NLP\\NLP PROJECTS\\Text_Summarization\\End_to_End_Text_Summarizer\\mlruns' does not exist."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
